## 进程和线程池案例

```python

import requests
from lxml import etree
from concurrent.futures import ThreadPoolExecutor
from urllib import parse
from multiprocessing import Process, Queue

header = {
    "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.60 Safari/537.36"}


def func1(url, q):
    resp = requests.get(url, headers=header)
    resp.encoding = resp.apparent_encoding
    # print(resp.text)
    tree = etree.HTML(resp.text)
    div_list = tree.xpath('//*[@id="popular-categories"]/div/ul/li/a/@href')
    for url_1 in div_list:
        url_2 = parse.urljoin(url, url_1)
        # print(url_2)
        resp2 = requests.get(url_2, headers=header)
        resp2.encoding = resp.apparent_encoding
        tree2 = etree.HTML(resp2.text)
        url_3 = tree2.xpath('//*[@id="main"]/div/a/img/@src')
        for url_4 in url_3:
            # print(url_4)
            print("数据放入列表")
            q.put(url_4)
        q.get("没了")


def func2(q):
    with ThreadPoolExecutor(10) as t:
        while True:
            img = q.get()
            if q.get() == "没了":
                break
            t.submit(func3, img)


def func3(url_4):
    file_name = url_4.split("/")[-1]
    print("数据正在下载")
    with open(fr"D:\新建python文件\1111\{file_name}", "wb") as f:
    f.write(requests.get(url_4).content)
    print("数据下载完成")


if __name__ == '__main__':
    q = Queue()
    url = "https://stocksnap.io/"
    p1 = Process(target=func1, args=(url, q))
    p2 = Process(target=func2, args=(q,))
    p1.start()
    p2.start()
```

用地多进程和线程池。

思路：

​		func1拿到图片下载地址后，交给func2后，func2交给func3，func3开始下载片到本地。(func1和func2是两个不同地进程，两个不同地进程要拿数据需要一个队列。就是q = Queue()，func1将数据放到队列，在又func2去队列拿数据。)

## 这个时协程的案例

### 案例一

```python
import aiohttp
import asyncio
import aiofiles


async def download(url):
    print("开始下载")
    try:
        name = url.split("/")[-1]
        # 创建session对象 -> 相当于requsts对象
        async with aiohttp.ClientSession() as session:
            # 发送请求, 这里和requests.get()几乎没区别, 除了代理换成了proxy
            async with session.get(url) as resp:
                # 读取数据. 如果想要读取源代码. 直接resp.text()即可. 比原来多了个()
                content = await resp.content.read()
                # 写入文件, 用默认的open也OK. 用aiofiles能进一步提升效率
                async with aiofiles.open(fr"D:\新建python文件\1111\{name}", mode="wb") as f:
                    await f.write(content)
                    print("下载完成")
                    return "OK"
    except:
        print("结束")
        return "NO"


async def main():
    url_list = [
    "https://img95.699pic.com/photo/50047/4567.jpg_wh860.jpg",
    "https://img95.699pic.com/photo/50059/7158.jpg_wh860.jpg",
    ]
    tasks = []
    for url in url_list:
        # 创建任务
        task = asyncio.create_task(download(url))
        tasks.append(task)
    await asyncio.wait(tasks)
    print("执行完了")



# asyncio.run(main())
event_loop = asyncio.get_event_loop()
event_loop.run_until_complete(main())

```

### 案例2

```python
"""
思路:1. 拿到"https://www.mingchaonaxieshier.com/"的原码,分析出每一节的url,
    2. 拿到所有分析出的url,用协程下载里面的章节.
    3.保存下载好的小说.
"""
import requests
from lxml import etree
import aiohttp
import asyncio
import aiofiles
import time

def url_one(url):
    while True:
        try:
            header = {
                "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.60 Safari/537.36"}
            resp = requests.get(url, headers=header)
            resp.encoding = resp.apparent_encoding
            # print(resp.text)
            tree = etree.HTML(resp.text)
            tbody = tree.xpath('/html/body/div[3]')
            url_3_list = []
            for href in tbody:
                url_1 = href.xpath('./div/div/center/table/tr/td/center/h2/a/@href')
                for i in url_1:
                    resp1 = requests.get(i, headers=header)
                    resp1.encoding = resp1.apparent_encoding
                    tree = etree.HTML(resp1.text)
                    url_3 = tree.xpath('/html/body/div[3]/div[2]/ul/li/a/@href')
                    for i in url_3:
                        url_3_list.append(i)
                        # print(i)
            return url_3_list
        except:
            print("错误了，重新开始")
            print("................................")
            time.sleep(3)


async def url_two(url_3_list):
    tast = []
    for url_2 in url_3_list:
        # print(url_2)
        t = asyncio.create_task(downlead(url_2))
        tast.append(t)
    await asyncio.wait(tast)
    print("运行结束")


async def downlead(url_2):
    while True:
        try:
            print("开始下载",url_2)
            async with aiohttp.ClientSession() as session:
                async with session.get(url_2) as resp:
                    page = await resp.text()
                    tree = etree.HTML(page)
                    # 开始解析，解析过程中不需要
                    name = ''.join(tree.xpath("/html/body/div[3]/h1/text()"))
                    text = '\n'.join(tree.xpath("/html/body/div[3]/div[2]/p//text()"))
                    # print(name)
                    # print(text)
                    async with aiofiles.open(fr"D:\新建python文件\明朝那些事\{name}.txt","w",encoding="utf-8") as f:
                        await f.write(text)
                        print("下载完成",url_2)
                        break
        except:
            print(url_2,"重写下载")

def main():
    url = "https://www.mingchaonaxieshier.com/"
    url_3_list= url_one(url)
    # asyncio.run(url_two(url_3_list))
    event_loop = asyncio.get_event_loop()
    event_loop.run_until_complete(url_two(url_3_list))


if __name__ == '__main__':
    main()
```

### 作业1

将<明朝那些事儿>的篇章进行分类.
要求: 每一部作为一个单独的文件夹进行存储.

```python
import asyncio
import aiofiles
import aiohttp
import requests
from lxml import etree
import os
import time


def get_all_detail_url(url):
    """
    获取到所有详情页的url
    :param url: 主页URL
    :return: {章节名称:[detail_url, detail_url....]}
    """
    resp = requests.get(url)
    tree = etree.HTML(resp.text)
    booklist = tree.xpath("//div[@class='booklist clearfix']/span")

    dic = {}
    chapter = ""
    for book in booklist:
        if 'v' in book.xpath("./@class"):
            chapter = book.xpath("./text()")[0]
            dic[chapter] = []
        else:
            href = book.xpath("./a/@href")[0]
            dic[chapter].append(href)
    return dic


async def download_one(session, file_path, url):
    async with session.get(url) as resp:
        text = await resp.text()
        tree = etree.HTML(text)
        title = tree.xpath(".//div[@class='chaptertitle clearfix']/h1/text()")[0]
        content = "\n".join(tree.xpath(".//div[@id='BookText']/text()")).replace("\u3000", "")
        async with aiofiles.open(f"./{file_path}/{title}.txt", mode="w", encoding='utf-8') as f:
            await f.write(content)


async def download(file_path, urls):
    tasks = []
    print(id(asyncio.get_event_loop()))
    async with aiohttp.ClientSession() as session:
        for url in urls:
            tasks.append(asyncio.create_task(download_one(session, file_path, url)))
        await asyncio.wait(tasks)
        print(file_path, "done")


def main():
    # 拿到目录页中所有详情页的url
    url = "https://www.zanghaihua.org/mingchaonaxieshier/"
    detail_urls = get_all_detail_url(url)

    for name, urls in detail_urls.items():
        if not os.path.exists(name):
            os.makedirs(f"./{name}/")
        asyncio.run(download(name, urls))


if __name__ == '__main__':
    start = time.time()
    main()
    print(time.time()-start)

```

### 案例三

函数merge_ts的将多个视频整合到一起的功能不能实现。

其他都是可以的

在协程函数下，也是可以不用async with aiofiles.open()这样就是多个一起打开，也可以用with open()一个一个打开。

自省也是很重要的。

```python
import requests
import aiohttp
import asyncio
import aiofiles
import re
from Crypto.Cipher import AES
import os


# 针对云播TV的分析:
程任务
#1.在https://pps.sdplay.com/20220423/IrDpzA0f/1200kb/hls/index.m3u8拿到数据
#2.用协程下载数据并保存数据。
#3.拿到钥匙，
# 4. 对ts文件进行解密操作: 先拿key   拿key时可以范访问key的url，也可以在上一个url中正则出来。
# 5. 对ts文件进行合并. 还原回mp4文件
def url_one(url):
    print("开始下载")
    while True:
        try:
            header = {
                "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.60 Safari/537.36"}
            resp = requests.get(url, headers=header)
            resp.encoding = resp.apparent_encoding
            print(resp.text)
            # print(resp.text)
            print("下载完成")
            return resp.text
        except:
            print("错误了，重新开始")


def downlerd(url_list):
    print("保存成功")
    with open(r"D:\新建python文件\ts.txt", "w", encoding="utf-8") as f:
        f.write(url_list)


async def downlead_all_ts():
    print("拿出数据")
    task = []
    with open(r"D:\新建python文件\ts.txt", "r", encoding="utf-8") as f:
        for ts in f:
            if ts.startswith("#"):
                continue
            # print(ts)
            ts = ts.strip()  # 必须处理  后面有换行符。将ts后面的换行符请掉。
            t = asyncio.create_task(downlead_all(ts))
            task.append(t)
    await asyncio.wait(task)


async def downlead_all(ts):
    print(ts, "开始加载")
    # 这个时自省   就是我们在一下发送请求时，服务器可能有几个没有返回的。要将这几个在重新发送。
    for i in range(10):
        try:
            name = ts.split("/")[-1]
            async with aiohttp.ClientSession() as Session:
                async with Session.get(ts) as resp:
                    content = await resp.content.read()
                    async with aiofiles.open(f"D:\新建python文件\未解密电影\{name}", "wb") as f:
                        await f.write(content)
            print(ts, "下载完成")
            break
        except:
            print("下载失败, 出现错误", ts)
            # time.sleep() # 可以适当的进行睡眠
            # 异步爬虫没必要
            await asyncio.sleep((i + 1) * 5)


def get_key():
    obj = re.compile(r'URI="(?P<key>.*?)"', re.S)
    with open(r'D:\新建python文件\ts.txt', "r", encoding="utf-8") as f:
        key = obj.search(f.read())
        key_url = key.group("key")
        print(key_url)
        # 请求到key的url, 获取到真正的秘钥
        key_url_request = url_one(key_url)
        return key_url_request.encode("utf-8")


async def des_all_ts_file(key):
    tasks = []
    with open(r"D:\新建python文件\ts.txt", mode="r", encoding='utf-8') as f:
        for line in f:
            if line.startswith("#"):
                continue
            line = line.strip() # line中有换行符，加strip()可以掉，不然循环出的line后会有\n。
            file_name = line.split("/")[-1]
            print(file_name)
            # 准备异步操作
            task = asyncio.create_task(des_one(file_name, key))
            tasks.append(task)
    await asyncio.wait(tasks)


async def des_one(file, key):
    print("即将开始解密", file)
    # 加密解密对象创建
    aes = AES.new(key=key, IV=b"0000000000000000", mode=AES.MODE_CBC)
    async with aiofiles.open(f"D:\新建python文件\未解密电影\{file}", mode="rb") as f1, \
            aiofiles.open(f"D:\新建python文件\解密后的\{file}", mode="wb") as f2:
        # 从加密后的文件中读取出来. 进行解密. 保存在未加密文件中
        content = await f1.read()
        bs = aes.decrypt(content)
        await f2.write(bs)
    print("文件已经解密", file)


# zhe
def merge_ts():
    name_list = []
    with open(r"D:\新建python文件\ts.txt", mode="r", encoding='utf-8') as f:
        for line in f:
            if line.startswith("#"):
                continue
            line = line.strip()
            file_name = line.split("/")[-1]
            name_list.append(file_name)
    # print(name_list)
    # 切换工作目录 到 ./电影_源_解密后/
    # 1.记录当前工作目录
    now_dir = os.getcwd()
    print(now_dir)
    # 2. 切换工作目录 到 ./电影_源_解密后/
    os.chdir(f"D:\新建python文件\解密后的")
    # 分而治之
    # 一次性合并100个文件
    temp = []
    n = 1
    for i in range(len(name_list)):
        name = name_list[i]
        temp.append(name)  # [a.ts, b.ts, c.ts]
        if i != 0 and i % 100 == 0:  # 每100个合并一次
            # 合并,
            # cat a.ts b.ts c.ts > xxx.mp4
            # copy /b a.ts + b.ts + c.ts xxx.mp4
            names = " ".join(temp)
            os.system(f"copy /b {names} {n}.ts")
            n += 1
            temp = []  # 还原成新的待合并列表
    # 把最后没有合并的进行收尾
    names = " ".join(temp)
    os.system(f"copy /b {names} {n}.ts")
    n += 1
    temp_2 = []
    # 把所有的n进行循环
    for i in range(1, n):
        temp_2.append(f"{i}.ts")

    names = " ".join(temp_2)
    os.system(f"copy /b {names} movie.mp4")

    # 3. 所有的操作之后. 一定要把工作目录切换回来
    os.chdir(now_dir)

def main():
    print("开始")
    url = "https://pps.sd-play.com/20220423/IrDpzA0f/1200kb/hls/index.m3u8"
    # url_list = url_one(url)
    # downlerd(url_list)
    # event_loop = asyncio.get_event_loop()
    # event_loop.run_until_complete(downlead_all_ts())
    # key = get_key()
    # event_loop = asyncio.get_event_loop()
    # event_loop.run_until_complete(des_all_ts_file(key))
    merge_ts()

if __name__ == '__main__':
    main()
```

# 5.文件写入的问题

```python
with open(r"D:\新建python文件\ts.txt","r") as f:
    print(f)
    content=f.read()
    print(content)
    # for循环写content，就会一个字母一个字母的输出。
    # 写f就和列表一样，s
    for i in f:
        print(i)
```

